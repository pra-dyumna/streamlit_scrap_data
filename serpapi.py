# from serpapi import GoogleSearch

# def get_search_results(keyword):
#     params = {
#         "engine": "google",
#         "q": keyword,
#         "api_key": "b39c63d4d19969f216fe515b23e526d3111a568d0aeb9280a01df11afb33711a",
#         "num": 10
#     }
#     search = GoogleSearch(params)
#     results = search.get_dict()
#     urls = [result['link'] for result in results['organic_results']]
#     return urls

# import requests, re, tldextract
# from bs4 import BeautifulSoup

# def scrape_website(url):
#     try:
#         resp = requests.get(url, timeout=10)
#         soup = BeautifulSoup(resp.text, 'html.parser')
#         print(f"soup: {soup}")
#         # Extract email and phone
#         text = soup.get_text()

#         print(f"text : {text}")
#         emails = re.findall(r"[a-zA-Z0-9_s.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", text)
#         phones = re.findall(r"\+?\d[\d\s\-\(\)]{7,}", text)
        
#         # Contact Page URL
#         contact_links = [a['href'] for a in soup.find_all('a', href=True) if 'contact' in a['href'].lower()]
#         contact_url = contact_links[0] if contact_links else None

#         # Domain
#         domain = tldextract.extract(url).top_domain_under_public_suffix
        
#         return {
#             "url": url,
#             "domain": domain,
#             "contact_url": contact_url,
#             "email": emails[0] if emails else None,
#             "phone": phones[0] if phones else None
#         }
#     except Exception as e:
#         print(f"Failed on {url}: {e}")
#         return None


# import csv

# def save_to_csv(data_list, filename='scraped_data.csv'):
#     keys = data_list[0].keys()
#     with open(filename, 'w', newline='', encoding='utf-8') as f:
#         writer = csv.DictWriter(f, fieldnames=keys)
#         writer.writeheader()
#         writer.writerows(data_list)


# keywords = ["digital marketing agencies in USA"]
# all_data = []

# for keyword in keywords:
#     urls = get_search_results(keyword)
#     for url in urls:
#         data = scrape_website(url)
#         if data:
#             all_data.append(data)

# save_to_csv(all_data)






import requests
from bs4 import BeautifulSoup
import re
import time
import random
import pandas as pd
from urllib.parse import urlparse, urljoin
from langchain_community.document_loaders import WebBaseLoader
import re
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 10_3 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) CriOS/56.0.2924.75 Mobile/14E5239e Safari/602.1"
]

# location_map = {
#     "usa": "us", "uk": "uk", "india": "in", "canada": "ca", "australia": "au"
# }

# def get_driver():
#     options = Options()
#     options.add_argument("--headless")
#     options.add_argument("start-maximized")
#     options.add_argument("disable-infobars")
#     options.add_argument("--disable-extensions")
#     options.add_argument("--disable-blink-features=AutomationControlled")
#     options.add_argument(f"user-agent={random.choice(user_agents)}")
#     return webdriver.Chrome(options=options)

def get_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("start-maximized")
    options.add_argument("disable-infobars")
    options.add_argument("--disable-extensions")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/114.0.0.0 Safari/537.36")
    return webdriver.Chrome(options=options)

location_map = {
    "usa": "us",
    "uk": "uk",
    "india": "in",
    "canada": "ca",
    "australia": "au",
}

# def search_google(query, location="usa", num_results=10):
#     gl = location_map.get(location.lower(), "us")
#     urls = []
#     driver = get_driver()
#     try:
#         print("üîç Google search...")
#         google_url = f"https://www.google.com/search?q={query.replace(' ', '+')}&num={num_results}&gl={gl}"
#         driver.get(google_url)
#         time.sleep(random.uniform(2, 5))
#         links = driver.find_elements(By.XPATH, '//a[@href]')
#         for link in links:
#             href = link.get_attribute("href")
#             if href and "/url?q=" in href:
#                 actual_url = href.split("/url?q=")[1].split("&")[0]
#                 domain = urlparse(actual_url).netloc
#                 urls.append((actual_url, domain))
#     except Exception as e:
#         print(f"‚ùå Google error: {e}")
#     driver.quit()
#     return urls


def search_google(query, location="usa", num_results=10):
    gl = location_map.get(location.lower(), "us")
    urls = []
    seen_urls = set()
    driver = get_driver()
    try:
        print("üîç Google search...")
        google_url = f"https://www.google.com/search?q={query.replace(' ', '+')}&num={num_results}&gl={gl}"
        driver.get(google_url)
        time.sleep(random.uniform(2, 5))

        links = driver.find_elements(By.XPATH, '//a[@href]')
        for link in links:
            href = link.get_attribute("href")
            if href and "/url?q=" in href:
                actual_url = href.split("/url?q=")[1].split("&")[0]
                if actual_url and actual_url not in seen_urls:
                    seen_urls.add(actual_url)
                    domain = urlparse(actual_url).netloc
                    urls.append((actual_url, domain))
    except Exception as e:
        print(f"‚ùå Google error: {e}")
    driver.quit()
    return urls

# def search_bing(query, location="usa", num_results=10):
#     urls = []
#     driver = get_driver()
#     try:
#         print("üîç Bing search...")
#         bing_url = f"https://www.bing.com/search?q={query.replace(' ', '+')}&count={num_results}"
#         driver.get(bing_url)
#         time.sleep(random.uniform(2, 5))
#         links = driver.find_elements(By.XPATH, '//li[@class="b_algo"]//h2/a')
#         for link in links:
#             href = link.get_attribute("href")
#             if href:
#                 actual_url = href
#                 domain = urlparse(actual_url).netloc
#                 urls.append((actual_url, domain))
#     except Exception as e:
#         print(f"‚ùå Bing error: {e}")
#     driver.quit()
#     return urls


def search_bing(query, location="usa", num_results=10):
    urls = []
    seen_urls = set()
    driver = get_driver()
    try:
        print("üîç Bing search...")
        bing_url = f"https://www.bing.com/search?q={query.replace(' ', '+')}&count={num_results}"
        driver.get(bing_url)
        time.sleep(random.uniform(2, 5))

        links = driver.find_elements(By.XPATH, '//li[@class="b_algo"]//h2/a')
        for link in links:
            href = link.get_attribute("href")
            if href and href not in seen_urls:
                seen_urls.add(href)
                domain = urlparse(href).netloc
                urls.append((href, domain))
    except Exception as e:
        print(f"‚ùå Bing error: {e}")
    driver.quit()
    return urls





def search_duckduckgo(query, location="usa", num_results=10):
    gl = location_map.get(location.lower(), "us")
    urls = []
    driver = get_driver()
    try:
        print("üîç DuckDuckGo search...")
        duck_url = f"https://duckduckgo.com/?q={query.replace(' ', '+')}&kl={gl}-en"
        driver.get(duck_url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.XPATH, '//a[@class="result__a"]'))
        )
        time.sleep(random.uniform(2, 5))
        links = driver.find_elements(By.XPATH, '//a[@class="result__a"]')
        for link in links[:num_results]:
            href = link.get_attribute("href")
            if href:
                actual_url = href
                domain = urlparse(actual_url).netloc
                urls.append((actual_url, domain))
    except Exception as e:
        print(f"‚ùå DuckDuckGo error: {e}")
    driver.quit()
    return urls


# def extract_info_with_langchain(url):
#     try:
#         # Step 1: Use LangChain to extract text from URL
#         loader = WebBaseLoader(url)
#         docs = loader.load()
#         text = docs[0].page_content if docs else ""

#         # Step 2: Extract emails
#         emails = set(re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", text))
#         email = ""
#         for e in emails:
#             if any(x in e.lower() for x in ["contact", "", "support", "email", "mail to"]):
#                 email = e
#                 break
#         if not email and emails:
#             email = list(emails)[0]

#         # Step 3: Extract phone numbers
#         phones = re.findall(r"(\+?\d[\d\s().-]{6,20}\d)", text)
#         phone = ""
#         valid_phones = []
#         for ph in phones:
#             clean = re.sub(r"\D", "", ph)
#             if 9 <= len(clean) <= 12:
#                 valid_phones.append(ph)
#         if valid_phones:
#             phone = valid_phones[0]

#         # Step 4: Try to find contact page from metadata or content
#         # Since LangChain's loader does not give anchor tags, you can still do a fallback fetch:
#         from bs4 import BeautifulSoup
#         import requests

#         contact_url = ""
#         try:
#             page = requests.get(url, timeout=10)
#             soup = BeautifulSoup(page.text, "html.parser")
#             for a in soup.find_all("a", href=True):
#                 href = a["href"].lower()
#                 if "contact" in href and not href.startswith("mailto:"):
#                     contact_url = urljoin(url, href)
#                     break
#         except:
#             pass

#         return {
#             "Website URL": url,
#             "Phone Number": phone,
#             "Email Address": email,
#             "Contact Us": contact_url,
#             "Error": ""
#         }

#     except Exception as e:
#         return {
#             "Website URL": url,
#             "Phone Number": "",
#             "Email Address": "",
#             "Contact Us": "",
#             "Error": str(e)
#         }
    

def extract_info_from_page(url):
    headers = {"User-Agent": random.choice(user_agents)}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return {"Website URL": url, "Error": f"HTTP {response.status_code}"}

        soup = BeautifulSoup(response.text, "html.parser")
        text = soup.get_text(separator=' ', strip=True)

        # Extract emails
        emails = set()

        # Match standard and slightly obfuscated email formats from text
        raw_emails = re.findall(r"[a-zA-Z0-9_.+-]+(?:\s*\[?@(?:at)?\]?\s*|\s*@\s*)[a-zA-Z0-9-]+(?:\.[a-zA-Z0-9-]+)+", text)

        for e in raw_emails:
            cleaned = e.replace("[at]", "@").replace("(at)", "@").replace(" at ", "@").replace(" AT ", "@")
            cleaned = re.sub(r"\s+", "", cleaned)
            emails.add(cleaned)

        # Also extract emails from <a href="mailto:..."> links
        for a in soup.find_all("a", href=True):
            href = a["href"].lower()
            if "mailto:" in href:
                email = href.split("mailto:")[1].split("?")[0].strip()
                emails.add(email)

        # Prioritize emails with common useful keywords
        priority_keywords = ["contact", "info", "support", "hello", "admin", "team", "mail", "email", "help", "service"]

        email = ""
        for e in emails:
            if any(k in e.lower() for k in priority_keywords):
                email = e
                break

# Fallback to first found email
        if not email and emails:
            email = list(emails)[0]

                # Extract phone numbers
        phones = re.findall(r"(\+?\d[\d\s().-]{6,20}\d)", text)
        phone = ""
        valid_phones = []
        for ph in phones:
            clean = re.sub(r"\D", "", ph)
            if 8 <= len(clean) <= 12:
                valid_phones.append(ph)
        if valid_phones:
            phone = valid_phones[0]

        # Find contact page
        contact_url = ""
        for a in soup.find_all("a", href=True):
            href = a["href"].lower()
            if "contact" in href and not href.startswith("mailto:"):
                contact_url = urljoin(url, href)
                break

        return {
            "Website URL": url,
            "Phone Number": phone,
            "Email Address": email,
            "Contact Us": contact_url,
           
        }

    except Exception as e:
        return {
            "Website URL": url,
            "Phone Number": "",
            "Email Address": "",
            "Contact Us": "",
            
        }



